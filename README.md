---
marp: true
title: Visualizing LLM Hallucinations
author: Anand S
url: https://sanand0.github.io/llmhallucinations/
theme: gaia
class: gaia
paginate: true
---

<!-- _backgroundColor: purple -->

# LLMs in Education

### [Anand S](https://s-anand.net/)

###### LLM Psychologist @ Straive

![h:180px](https://api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://sanand0.github.io/llms-in-education/)

Slides: [sanand0.github.io/llms-in-education](https://sanand0.github.io/llms-in-education)

---

## How Do We Use LLMs in Education?

- **LLMs are new**. No one really knows how to use them in education. Let's explore.
- My examples are **starting points**—your thoughts will shape fresh ideas.
- Your inputs and experiments will give me and others new ideas. So please **ask questions and share ideas**—as many as you can.

---

## Opening Questions

1) What benefits and STRUGGLES have students reported in using LLMs while studying?
2) What differences do you see between students who use LLMs actively versus those that do not? (positive and negative)
3) What LLM-based techniques have you tried applying in your courses?

---

## Redesign Curriculum

- Fed my curriculum to an LLM.
- It flagged emerging topics and outdated ones.
- Discovered new tools: MarkitDown is better than Python-based PDF parsers.
- My courses stay fresh and tech-relevant.

---

## Write Course Content

- Generated FastAPI content from scratch—revealed gaps but not in my style.
- Switched to Cursor with Cloud 3.5 Sonnet using my best material.
- Got content in my voice with copy-paste code snippets.
- AI-picked YouTube links were concise, relevant, and popular.
- Created new topics like DuckDB and voice transcription in just 13 minutes.

---

## Automate Personalized Feedback

- LLMs evaluated code formatting, variable clarity, logic flow, modularity, and more.
- They provided reasons for each criterion, ending with a clear yes/no verdict.
- Feedback was granular—with specific examples on strengths and gaps.
- Performance matched human instructors but worked tirelessly.
- Automated evaluation freed up instructor time for higher-level tasks.

---

## Detect Copying

- Removed comments and normalized whitespace.
- Converted code into embeddings to gauge similarity.
- Mapped networks to flag potential copying.

---

## Insights from Copying Patterns

- Over half the students refrained from copying.
- Non-copiers performed the worst.
- Late copiers
- Early sharers (inviting others to copy) excelled
- Copying late is better than copying early.

---

## Transcribe Lectures

- Gemini can transcribe, correct & summarise in one step (vs Whisper transcription)
- Converted live audio into FAQs.
- Integrated transcriptions into course content for on-demand answers.
- Enabled quick navigation to specific topics and problem solutions.
- Contributed to a surge in live session attendance.

---

## Create a Virtual Instructor

- Converted all course materials into Markdown.
- Trained a custom GPT on the entire course content.
- Guided it with broad instructor-style instructions.
- Used a few thousand times, though its impact remains unproven.
- Envisioned integration into Discord for auto Q&A.

---

## Teach Prompt Engineering

- Challenged students to coax a “yes” from LLMs despite built-in restrictions.
- Explored creative prompts to bypass safeguard blocks.

---

## Add LLM-Powered Projects

- Assigned a project where LLMs generated code for diverse problem statements.
- Taught students to manage LLMs’ natural unreliability.
- Demonstrated LLMs’ ability to handle diverse tasks with minimal instructions.

---

## Automate Evaluations

- Passed a sample evaluation codebase to Cursor.
- Prompted generation of new evaluation questions in the same style.
- Created real-life use cases to contextualize the tasks.
- Helped students understand the practical relevance of their work.

---

## Things We Are Going To Try

- Evaluate subjective answers with LLMs.
- Analyze student mark distributions.
- Create personalized learning paths and virtual assistants.
- Enable students to craft their own course material.
- Leverage Discourse as a database for repetitive questions.
- Provide live assistance by capturing students’ coding screens.
- Build test cases for evaluations and quizzes.

---

<!-- _backgroundColor: purple -->

## Takeaways for Instructors

- **Modernize Curriculum:**
  add emerging tech trends, and phase out outdated topics.
- **Scale Impact with Automated Feedback:**
  Use AI-driven evaluations & code reviews for detailed, criterion-based feedback.
- **Pilot AI Initiatives:**
  Run experiments. Track engagement. Refine your approach.

Slides: [sanand0.github.io/llms-in-education](https://sanand0.github.io/llms-in-education)

![bg right:30% fit](https://api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://sanand0.github.io/llms-in-education/)

---

## Discussion & Next Steps

1) Which of the above LLM applications—from curriculum design to automated evaluations—do you believe holds the highest potential to transform learning experiences, and why?
2) Based on our discussion, what ideas come to your mind that you would like to try out?
